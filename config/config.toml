[env]
project_name = "Polymorphic Data Reporter"
timezone = "America/Chicago"
seed = 42
theme = "dark_blue"

[storage.local]
enabled = true
raw_root = "data/raw"
gold_root = "data/gold"

[storage.minio]
enabled = true
endpoint = "http://minio:9000"
access_key = "admin"
secret_key = "admin"
secure = false
bucket = "datasets"
raw_prefix = "raw/"
gold_prefix = "gold/"

[auth]
# If Airflow UI or any optional service needs creds in-app
username = "admin"
password = "admin"

[sources]
# discover from both local and MinIO
locations = [
  "file://data/raw/",
  "s3://datasets/raw/iot_sensors/"
]

[duckdb]
persist = true  # physical .duckdb written under data/gold/<slug>/tables/

[profiling.roles]
cat_cardinality_max = 120
datetime_formats = ["%Y-%m-%d", "%m/%d/%Y", "%Y-%m-%d %H:%M:%S"]

[profiling.outliers]
method = "zscore"         # zscore | iqr
zscore_threshold = 3.0
iqr_multiplier = 1.5

# ---------- NLP / NLG ----------

[nlp]
sample_rows = 200000
min_schema_confidence = 0.75
min_role_confidence = 0.75
max_iter = 5
min_improvement = 0.0
enable_domain_templates = true
granularity = "slug"  # slug | subdir | file

[nlp.role_scoring]
name_weight = 0.45
value_weight = 0.55

# thresholds
bool_token_min_ratio = 0.57     # share of values that look boolean
date_parse_min_ratio = 0.60     # share of values that parse as dates
unique_id_ratio = 0.95          # uniqueness ratio to consider "id-ish"
categorical_max_unique_ratio = 0.02
text_min_avg_len = 8            # avg string length to lean "text" vs "categorical"
min_non_null_ratio = 0.10       # ignore columns with less data than this

# bonuses/penalties
bonus_id_name = 0.10            # extra if name strongly looks like id (e.g., *_id)
penalize_bool_for_many_tokens = 0.08  # slight penalty if too many distinct tokens for bool

[nlg.constants]
inventory_key = "_inventory"
narrative_filename = "narrative.txt"

# ---------- Dynamic Cleaning Policy (highly flexible) ----------

[cleaning.columns]
drop_missing_pct      = 0.90
min_unique_ratio      = 0.001
always_keep           = ["id", "date", "timestamp"]
cat_cardinality_max   = 200

# Optional name-based pruning helpers (used by the drop:* rules below)
droppable_exact       = ["_source_uri", "batch_id"]
droppable_prefixes    = ["_tmp", "_raw", "tmp_", "raw_"]
droppable_suffixes    = ["_hash", "_checksum"]

[cleaning.normalize]
strip_text            = true
lowercase_text        = false
standardize_dates     = true
enforce_categorical   = true

[cleaning.impute]
numeric_default             = "median"    # mean|median|ffill|bfill|interpolate
categorical_default         = "Unknown"
text_default                = "N/A"
boolean_default             = false
time_aware_interpolation    = true

[cleaning.outliers]
detect            = "zscore"             # zscore|iqr
zscore_threshold  = 3.0
iqr_multiplier    = 1.5
handle            = "flag"               # flag|winsorize|drop
winsor_limits     = [0.01, 0.99]

[columns.X.role_confidence]
role = "numeric"
confidence = 0.93

# Centralized “null-like” tokens (used twice: at start & near the end as a guard)
[cleaning.normalize_null_tokens]
null_tokens = ["", "NA", "N/A", "N\\A", "None", "NULL", "NaN", "-", "—", "<NA>", "<Null>", "<None>", "nil", "missing"]

# Datetime parsing preferences (used by robust parser and/or fallback)
[cleaning.datetime]
utc        = true
dayfirst   = false
yearfirst  = false
# If your pipeline provides a dynamic set of formats (e.g., profiling), keep `datetime_formats` in env.
# Otherwise populate this static list and call parse_datetime(cleaning.datetime.formats)
formats    = ["%Y-%m-%d", "%m/%d/%Y", "%Y/%m/%d", "%Y-%m-%d %H:%M:%S", "%m/%d/%Y %H:%M:%S"]

# Regex presets for common cleanups
[cleaning.regex]
# Normalize any Unicode dash/minus variants to ASCII hyphen-minus for numeric parsing
unicode_minus            = "[-−-–—]"
# Strip common thousands separators *only* when clearly between digits
thousands_between_digits = "(?<=\\d)[,_](?=\\d{3}\\b)"

# ID defaults
[cleaning.ids]
zip_width        = 5
pad_fillchar     = "0"
pad_numeric_only = true


# =========================================
# Rules: ordered, with conservative guards
# =========================================

# 1) Normalize null-like tokens ASAP for strings
[[cleaning.rules]]
id = "normalize-nulls-first"
priority = 130
when = 'type == "string"'
then = 'normalize_null_tokens(null_tokens=cleaning.normalize_null_tokens.null_tokens, case_insensitive=true)'

# 2) Normalize Unicode minus/dash to ASCII hyphen BEFORE numeric coercion
[[cleaning.rules]]
id = "num:normalize-unicode-minus"
priority = 125
when = 'type == "string" and role in ["numeric","id"]'
then = 'regex_replace(pattern=cleaning.regex.unicode_minus, repl="-")'

# 2b) Remove thousands separators between digits prior to numeric coercion
[[cleaning.rules]]
id = "num:strip-thousands"
priority = 123
when = 'type == "string" and role in ["numeric","id"]'
then = 'regex_replace(pattern=cleaning.regex.thousands_between_digits, repl="")'

# 3) Coerce numeric (but never touch id-like names even if mis-scored as numeric)
[[cleaning.rules]]
id = "coerce-numeric"
priority = 120
when = 'role == "numeric" and type == "string" and not icontains(name,"id")'
then = 'coerce_numeric()'

# 4) Heuristic numeric coercion for shortish numeric-looking strings (also honors num_* prefix)
[[cleaning.rules]]
id = "coerce-numeric-from-string-heuristic"
priority = 118
when = '''
  type == "string"
  and role notin ["time","categorical","text"]
  and not istartswith(name,"cat_")
  and numeric_like_ratio != null and numeric_like_ratio >= 0.92
  and (unique_ratio == null or unique_ratio <= 0.50)
  and (avg_len == null or avg_len <= 18)
'''
then = 'coerce_numeric()'

# 5) Text normalization (trim + optional lower) for text fields
[[cleaning.rules]]
id = "normalize-text"
priority = 110
when = 'role == "text" and type == "string"'
then = 'text_normalize(strip=cleaning.normalize.strip_text, lower=cleaning.normalize.lowercase_text)'

# 6) Generic text normalization for text/categorical if a bit noisy
[[cleaning.rules]]
id = "text:norm"
priority = 105
when = "(role == 'categorical' or role == 'text') and avg_len != null and avg_len >= 1"
then = "text_normalize(strip=true, lower=true)"

# 7) Normalize null-like tokens again after text normalization (both cat & text)
[[cleaning.rules]]
id = "text:null-tokens"
priority = 104
when = "(role == 'categorical' or role == 'text')"
then = "normalize_null_tokens(null_tokens=cleaning.normalize_null_tokens.null_tokens, case_insensitive=true)"

# 8) Coerce booleans from tokens (yes/no, true/false, y/n, 0/1, etc.)
[[cleaning.rules]]
id = "bool:tokens"
priority = 126
when = "bool_token_ratio >= 0.57 or istartswith(name,'bool_')"
then = "coerce_bool()"

# 8b) Fill missing booleans with the configured default (False)
[[cleaning.rules]]
id = "bool:impute-default"
priority = 124
when = 'role == "boolean" and missing_pct > 0'
then = 'impute_value(cleaning.impute.boolean_default)'

# 9) Categorical consolidation (collapse rare labels)
[[cleaning.rules]]
id = "cat:rare-collapse"
priority = 95
when = "role == 'categorical' and unique_ratio > 0 and unique_ratio <= 0.50"
then = "rare_cats(0.01, 'Other')"

# 10) Cast to pandas 'category' if small enough
[[cleaning.rules]]
id = "cat:cast-small"
priority = 90
when = "role == 'categorical' and cardinality <= cleaning.columns.cat_cardinality_max"
then = "cast_category()"

# 11) Targeted imputation policies
[[cleaning.rules]]
id = "impute-numeric-time"
priority = 88
when = 'role == "numeric" and missing_pct > 0 and has_time_index'
then = 'impute("interpolate")'

[[cleaning.rules]]
id = "impute-numeric-default"
priority = 86
when = 'role == "numeric" and missing_pct > 0'
then = 'impute(numeric_default)'

[[cleaning.rules]]
id = "impute-categorical"
priority = 86
when = 'role == "categorical" and type == "string" and missing_pct > 0'
then = 'impute_value(categorical_default)'

[[cleaning.rules]]
id = "impute-text"
priority = 84
when = 'role == "text" and type == "string" and missing_pct > 0 and avg_len >= 8'
then = 'impute_value(text_default)'

# 12) Outliers – flag or winsorize
[[cleaning.rules]]
id = "flag-outliers"
priority = 80
when = 'role == "numeric"'
then = 'outliers(detect, zscore_threshold, iqr_multiplier, handle, winsor_limits)'

[[cleaning.rules]]
id = "outliers:winsorize"
priority = 78
when = "role == 'numeric' and std != null and std > 0 and cleaning.outliers.handle == 'winsorize'"
then = "outliers(method=detect, zscore_threshold=zscore_threshold, iqr_multiplier=iqr_multiplier, handle='winsorize', winsor_limits=winsor_limits)"

# 13) Units – infer/standardize percents
[[cleaning.rules]]
id = "units:percent-by-name"
priority = 76
when = "role == 'numeric' and (icontains(name, 'percent') or icontains(name, 'pct'))"
then = "standardize_units('percent')"

[[cleaning.rules]]
id = "units:percent-by-range"
priority = 74
when = "role == 'numeric' and min >= 0 and max <= 100 and max > 1"
then = "standardize_units('percent')"

# 14) Strong numeric policies by name (nonnegative amounts/prices)
[[cleaning.rules]]
id = "num:nonnegative-by-name"
priority = 70
when = "role == 'numeric' and (icontains(name, 'amount') or icontains(name, 'price') or icontains(name, 'cost'))"
then = "enforce_sign('nonnegative')"

# 15) Treat zeros as missing for ratios/rates/percents
[[cleaning.rules]]
id = "num:zero-as-missing"
priority = 68
when = "role == 'numeric' and (icontains(name, 'ratio') or icontains(name, 'rate') or icontains(name, 'percent') or icontains(name, 'pct'))"
then = "zero_as_missing()"

# 16) Datetime parsing for string fields: try explicit formats first, then robust parser
[[cleaning.rules]]
id = "dt:parse-formats"
priority = 90
when = "role == 'time' and type == 'string'"
then = "parse_datetime(datetime_formats)"

[[cleaning.rules]]
id = "dt:parse-robust"
priority = 88
when = "role == 'time' and type == 'string'"
then = "parse_datetime_robust(utc=cleaning.datetime.utc, dayfirst=cleaning.datetime.dayfirst, yearfirst=cleaning.datetime.yearfirst)"

# 17) Parse epoch (seconds/ms/us/ns) for numeric timestamp columns
[[cleaning.rules]]
id = "dt:parse-epoch"
priority = 86
when = "role == 'time' and type in ['int','float']"
then = "parse_epoch()"

# 18) Optional rounding/floor/ceil for grouping
[[cleaning.rules]]
id = "dt:round-day"
priority = 60
when = "role == 'time'"
then = "dt_round('D')"

# 19) Time imputation policies
[[cleaning.rules]]
id = "impute-time-forward"
priority = 82
when = 'role == "time" and missing_pct > 0 and has_time_index'
then = 'impute_dt("ffill")'

[[cleaning.rules]]
id = "impute-time-default"
priority = 80
when = 'role == "time" and missing_pct > 0 and not has_time_index'
then = 'impute_dt("median")'

# 20) ID hygiene – ZIP: alnum only, then zero-pad (numeric_only so “SW1A” won’t get mangled)
[[cleaning.rules]]
id = "id:zip-clean"
priority = 72
when = "(icontains(name, 'zip') or icontains(name, 'postal'))"
then = "keep_alnum()"

[[cleaning.rules]]
id = "id:zip-pad"
priority = 71
when = "(icontains(name, 'zip') or icontains(name, 'postal'))"
then = "zero_pad(cleaning.ids.zip_width, cleaning.ids.pad_fillchar, numeric_only=cleaning.ids.pad_numeric_only)"

# 21) Phone numbers: strip to digits only
[[cleaning.rules]]
id = "id:phone-digits"
priority = 72
when = "(icontains(name, 'phone') or icontains(name, 'tel')) and type == 'string'"
then = "extract_digits()"

# 22) Final text NA guard & materialization (ensures no bare empty strings survive)
[[cleaning.rules]]
id = "materialize-missing-text-literal"
priority = 52
when = 'type == "string" and missing_pct > 0'
then = 'materialize_missing_as(cleaning.impute.text_default)'

[[cleaning.rules]]
id = "final-null-guard-text"
priority = 51
when = 'type == "string" and missing_pct > 0'
then = 'impute_value(text_default)'

[[cleaning.rules]]
id = "normalize-nulls-last"
priority = 50
when = 'type == "string"'
then = 'normalize_null_tokens(null_tokens=cleaning.normalize_null_tokens.null_tokens, case_insensitive=true, apply_text_normalize_first=false)'

# 23) Name-based pruning (safe without custom DSL helpers)
[[cleaning.rules]]
id = "drop:name-noise-exact"
priority = 45
when = "name in cleaning.columns.droppable_exact"
then = "drop_column()"

[[cleaning.rules]]
id = "drop:name-noise-prefix"
priority = 45
when = "istartswith(name,'_tmp') or istartswith(name,'_raw') or istartswith(name,'tmp_') or istartswith(name,'raw_')"
then = "drop_column()"

[[cleaning.rules]]
id = "drop:name-noise-suffix"
priority = 45
when = "icontains(name,'_hash') or icontains(name,'_checksum')"
then = "drop_column()"

# 24) Pruning: sparse & constant (unless allow-listed)
[[cleaning.rules]]
id = "drop-sparse"
priority = 40
when = "missing_pct >= cleaning.columns.drop_missing_pct and role notin ['boolean','text','categorical','id','time'] and name notin cleaning.columns.always_keep"
then = 'drop_column()'

[[cleaning.rules]]
id = "drop-constant"
priority = 40
when = "unique_ratio <= cleaning.columns.min_unique_ratio and role notin ['boolean','text','categorical','id','time'] and name notin cleaning.columns.always_keep"
then = 'drop_column()'

# ---------- Topic Selection & Charts ----------

[topics.thresholds]
min_corr_for_scatter = 0.35
min_slope_for_trend = 0.02
max_categories_bar = 20
max_series_line = 8
max_charts_total = 12

[charts]
max_charts_per_topic = 6
facet_max_series = 8
topk_categories = 20
prefer_small_multiples = true
allow_pie_when_n_le = 5
enable_advanced = ["treemap","sankey","calendar_heatmap","parcoords"]
export_static_png = true

[weights]
suitability = 3.0
effect_size = 2.5
signal_quality = 2.0
readability = 1.5
complexity = 1.0

# ---------- Output & Orchestration ----------

[reports.enabled_generators]
csv = true
json = true
parquet = true
charts = true
html = true
pdf = true

[reports.html]
template = "base.html"
title_prefix = "Report:"
embed_interactive = true

[reports.pdf]
engine = "chromium"          # chromium
page_size = "Letter"
margins = "0.5in"

[airflow]
dag_id = "polymorphic_report_dag"
schedule = "0 2 * * *"
max_active_runs = 1
catchup = false
concurrency = 8
task_retries = 2
username = "admin"
password = "admin"

[docker]
compose_file = "docker/docker-compose.yml"
airflow_image = "local/airflow:latest"
chrome_image = "local/chrome:latest"

[logging]
level = "INFO"
structured_json = true

[publishing]
enabled = false
target = "s3://datasets/gold-exports/"
access_key = "admin"
secret_key = "admin"
